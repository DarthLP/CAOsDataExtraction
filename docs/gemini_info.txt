================================================================================
Gemini - Basic Model Limits and Notes: https://ai.google.dev/gemini-api/docs and https://ai.google.dev/gemini-api/docs/models#gemini-2.5-pro
================================================================================

Input token limit: 1,048,576
Output token limit: 65,536

Rate limits (Text-out models):
Model                RPM   TPM       RPD
Gemini 2.5 Pro       5     250,000   100
Gemini 2.5 Flash     10    250,000   250

Token approximation:
- 1 token ≈ 4 characters (for Gemini models)
- 100 tokens ≈ 60–80 English words

================================================================================
THINKING CAPABILITIES - Gemini 2.5 Series Models: https://ai.google.dev/gemini-api/docs/thinking
================================================================================

What is Thinking?
Thinking is an advanced reasoning capability in Gemini 2.5 series models that allows the model to engage in internal reasoning before providing a response. The model can break down complex tasks, consider multiple approaches, and arrive at more thoughtful conclusions.

How to Implement Thinking:

1. Basic Implementation:
```python
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="Your prompt here",
    config=types.GenerateContentConfig(
        thinking_config=types.ThinkingConfig(thinking_budget=1024)
    )
)
```

2. Thinking Budget Options:
- Default: Dynamic thinking (model decides when and how much to think)
- Disable: thinking_budget=0
- Dynamic: thinking_budget=-1 (model adjusts based on complexity)
- Custom: thinking_budget=128 to 32768 (2.5 Pro) or 0 to 24576 (2.5 Flash)

3. Thought Summaries (Optional):
```python
config=types.GenerateContentConfig(
    thinking_config=types.ThinkingConfig(
        thinking_budget=1024,
        include_thoughts=True  # Get reasoning insights
    )
)
```

4. Task Complexity Guidelines:
- Easy Tasks: thinking_budget=0 (fact retrieval, classification)
- Medium Tasks: Default thinking (comparisons, analogies)
- Hard Tasks: High thinking budget (complex math, coding, reasoning)

5. Best Practices:
- Review thought summaries for debugging
- Provide guidance in prompts to constrain thinking
- Use with all Gemini tools (search, code execution, structured output)
- Balance thinking budget with response length needs

================================================================================
TEXT GENERATION - Basic API Usage and Configuration: https://ai.google.dev/gemini-api/docs/text-generation
================================================================================

What is Text Generation?
Text generation is the core capability of Gemini models to produce text output from various inputs (text, images, video, audio). The API provides extensive configuration options to control output quality, format, and behavior.

How to Implement Basic Text Generation:

1. Basic Implementation:
```python
from google import genai

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="How does AI work?"
)
print(response.text)
```

2. System Instructions and Configuration:
```python
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    config=types.GenerateContentConfig(
        system_instruction="You are a helpful assistant.",
        temperature=0.1
    ),
    contents="Hello there"
)
```

3. Deterministic Output (Same Input = Same Output):
```python
config=types.GenerateContentConfig(
    temperature=0.0,  # Zero randomness
    seed=12345,       # Fixed seed for reproducibility
    top_p=1.0,        # No nucleus sampling
    top_k=1           # Only consider top token
)
```

4. Structured Output (JSON):
```python
config=types.GenerateContentConfig(
    response_mime_type="application/json",
    response_schema=your_pydantic_model,
    temperature=0.1   # Lower temperature for consistency
)
```

Key Configuration Parameters:

TEMPERATURE & RANDOMNESS:
- temperature: 0.0-2.0 (0.0 = deterministic, 2.0 = very random)
- seed: Integer for reproducible results: When seed is fixed to a specific value, the model makes a best effort to provide the same response for repeated requests. Deterministic output isn't guaranteed. Also, changing the model or parameter settings, such as the temperature, can cause variations in the response even when you use the same seed value. By default, a random seed value is used.
- top_p: 0.0-1.0 (nucleus sampling)
- top_k: Integer (top-k sampling)

OUTPUT CONTROL:
- max_output_tokens: Maximum response length
- candidate_count: Number of responses to generate
- stop_sequences: Array of strings to stop generation

STRUCTURED OUTPUT:
- response_mime_type: "application/json", "text/plain", etc.
- response_schema: Pydantic model for structured output
- response_json_schema: Alternative JSON schema format

ADVANCED FEATURES:
- presence_penalty: Discourage token reuse
- frequency_penalty: Penalize repeated tokens
- response_logprobs: Include probability data
- thinking_config: Enable reasoning capabilities

Best Practices for Deterministic Output:
1. Set temperature=0.0
2. Use a fixed seed value
3. Set top_p=1.0 and top_k=1
4. Use consistent system instructions
5. Avoid presence/frequency penalties
6. For structured output, use low temperature (0.1-0.3)

================================================================================
STRUCTURED OUTPUT - JSON Generation and Schema Configuration: https://ai.google.dev/gemini-api/docs/structured-output
================================================================================

What is Structured Output?
Structured output constrains Gemini to generate JSON or enum values instead of unstructured text, enabling precise extraction and standardization of information for further processing. This is ideal for building structured databases, extracting information from documents, and standardizing data formats.

How to Implement Structured Output:

1. Basic JSON Generation with Pydantic:
```python
from google import genai
from pydantic import BaseModel

class Recipe(BaseModel):
    recipe_name: str
    ingredients: list[str]

client = genai.Client()
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="List a few popular cookie recipes with ingredients.",
    config={
        "response_mime_type": "application/json",
        "response_schema": list[Recipe],
    },
)

# Access as JSON string
print(response.text)

# Access as instantiated objects
my_recipes: list[Recipe] = response.parsed
```

2. Schema Configuration Options:
```python
config = {
    "response_mime_type": "application/json",
    "response_schema": your_pydantic_model,
    # Alternative: Use JSON Schema directly
    # "response_json_schema": json_schema_dict
}
```

3. Property Ordering (Critical for Consistency):
```python
# In your Pydantic model
class CAOExtractionSchema(BaseModel):
    model_config = ConfigDict(
        title="CAO Extraction Schema",
        # Ensure consistent property ordering
        json_schema_extra={
            "propertyOrdering": ["field1", "field2", "field3"]
        }
    )
```

Supported Schema Types:
- Basic: int, float, bool, str
- Collections: list[AllowedType], dict[str, AllowedType]
- Unions: AllowedType|AllowedType|...
- Custom: User-defined Pydantic models
- Nested: Complex nested structures

Schema Field Constraints:
- string: enum, format, nullable
- integer: format, minimum, maximum, enum, nullable
- number: format, minimum, maximum, enum, nullable
- boolean: nullable
- array: minItems, maxItems, items, nullable
- object: properties, required, propertyOrdering, nullable

Best Practices for Structured Output:

1. SCHEMA DESIGN:
   - Keep property names short and clear
   - Use propertyOrdering for consistent output
   - Flatten deeply nested arrays when possible
   - Limit enum values to reasonable numbers

2. COMPLEXITY MANAGEMENT:
   - Avoid overly complex schemas (causes 400 errors)
   - Reduce optional properties
   - Limit array length constraints
   - Simplify property constraints

3. VALIDATION:
   - Pydantic validators are not yet supported
   - Validation errors are suppressed
   - response.parsed may be empty if validation fails
   - Always check response.parsed before using

4. TROUBLESHOOTING:
   - Test without structured output first
   - Review model's natural response format
   - Adjust schema to match expected output
   - Add more context to input prompts

5. PERFORMANCE:
   - Schema size counts toward input token limit
   - Complex schemas increase processing time
   - Balance detail vs. performance needs

================================================================================
PROMPT DESIGN STRATEGIES - Best Practices and Techniques: https://ai.google.dev/gemini-api/docs/prompting-strategies
================================================================================

What is Prompt Design?
Prompt design is the process of creating natural language requests that elicit accurate, high-quality responses from language models. Effective prompts can significantly improve model performance and consistency.

Key Prompt Design Strategies:

1. CLEAR AND SPECIFIC INSTRUCTIONS:
   - Use explicit, unambiguous language
   - Break complex tasks into step-by-step instructions
   - Specify exact output formats and constraints
   - Example: "Summarize this text in exactly 3 bullet points"

2. INPUT TYPES:
   - Question Input: "What's a good name for a flower shop?"
   - Task Input: "Give me a list of 5 camping essentials"
   - Entity Input: "Classify these items as [large, small]: Elephant, Mouse"
   - Completion Input: Provide partial content for model to complete

3. CONSTRAINTS AND FORMATTING:
   - Specify response length: "in one sentence"
   - Define output format: "as a table", "as JSON", "as bullet points"
   - Set constraints: "only use the provided information"
   - Example: "Return only the numbers, no explanations"

4. FEW-SHOT VS ZERO-SHOT PROMPTING:
   - Zero-shot: No examples provided
   - Few-shot: Include 2-5 examples showing desired output format
   - Always prefer few-shot for better consistency
   - Use varied, specific examples to guide the model

5. CONTEXT AND BACKGROUND:
   - Provide relevant context and constraints
   - Include necessary information the model needs
   - Add domain-specific knowledge when required
   - Example: Include troubleshooting guides for specific devices

6. PREFIXES AND FORMATTING:
   - Input Prefix: "Text:", "Question:", "Context:"
   - Output Prefix: "JSON:", "Answer:", "Summary:"
   - Example Prefix: "Example 1:", "Example 2:"
   - Use consistent formatting across examples

7. PROMPT COMPONENTS BREAKDOWN:
   - Break complex prompts into simpler components
   - Chain prompts for multi-step tasks
   - Aggregate responses from parallel operations
   - Use system instructions for behavior guidance

8. MODEL PARAMETERS EXPERIMENTATION:
   - Temperature: 0.0 (deterministic) to 2.0 (very random)
   - Max Output Tokens: Control response length
   - Top-K: Select from top K most probable tokens
   - Top-P: Nucleus sampling threshold
   - Stop Sequences: Define when to stop generation

9. ITERATION STRATEGIES:
   - Try different phrasings for same request
   - Switch to analogous tasks if needed
   - Change content order in prompts
   - Use multiple choice format for classification
   - Increase temperature if getting fallback responses

10. BEST PRACTICES:
    - Always include few-shot examples when possible
    - Use positive patterns, not anti-patterns
    - Maintain consistent formatting across examples
    - Provide specific constraints and requirements
    - Test with different parameter combinations
    - Avoid relying on models for factual information
    - Use care with math and logic problems

11. DETERMINISTIC OUTPUT TECHNIQUES:
    - Set temperature=0.0 for maximum consistency
    - Use fixed seed values
    - Provide very specific instructions
    - Include detailed examples
    - Use structured output schemas
    - Set top_p=1.0 and top_k=1

12. COMMON PITFALLS TO AVOID:
    - Vague or ambiguous instructions
    - Inconsistent example formatting
    - Too many examples (overfitting)
    - Relying on model for facts
    - Not specifying output format
    - Using anti-patterns instead of positive examples

================================================================================
CONTENT GENERATION - API Methods and Response Handling: https://ai.google.dev/api/generate-content#v1beta.GenerationConfig
================================================================================

What is Content Generation?
Content generation is the core capability of the Gemini API to generate responses from various input types including text, images, audio, video, PDFs, and more. The API provides comprehensive methods for both single-turn and streaming responses.

Key API Methods:

1. BASIC CONTENT GENERATION:
```python
from google import genai

client = genai.Client()
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="Write a story about a magic backpack."
)
print(response.text)
```

2. STREAMING CONTENT GENERATION:
```python
response = client.models.generate_content_stream(
    model="gemini-2.5-flash",
    contents="Write a story about a magic backpack."
)
for chunk in response:
    print(chunk.text)
```

3. COMPLETE REQUEST STRUCTURE:
```python
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=[{"text": "Your prompt here"}],
    tools=[tool_config],  # Optional: Function calling, code execution
    tool_config=tool_config,  # Optional: Tool configuration
    safety_settings=[safety_setting],  # Optional: Safety filters
    system_instruction={"text": "System prompt"},  # Optional
    generation_config=generation_config,  # Optional: Generation parameters
    cached_content="cachedContents/name"  # Optional: Use cached content
)
```

Response Structure:

1. GENERATECONTENTRESPONSE:
   - candidates[]: Array of generated responses
   - prompt_feedback: Safety and content filter feedback
   - usage_metadata: Token usage information
   - model_version: Model version used
   - response_id: Unique response identifier

2. CANDIDATE RESPONSE:
   - content: Generated content object
   - finish_reason: Why generation stopped
   - safety_ratings: Safety assessment
   - citation_metadata: Source attributions
   - token_count: Number of tokens generated
   - grounding_attributions: Source references
   - avg_logprobs: Average probability score
   - logprobs_result: Detailed probability data

3. FINISH REASONS:
   - STOP: Natural completion
   - MAX_TOKENS: Token limit reached
   - SAFETY: Safety filter triggered
   - RECITATION: Copyright concerns
   - LANGUAGE: Unsupported language
   - OTHER: Unknown reason
   - BLOCKLIST: Forbidden terms
   - PROHIBITED_CONTENT: Content violations

4. USAGE METADATA:
   - prompt_token_count: Input tokens
   - candidates_token_count: Output tokens
   - total_token_count: Total tokens
   - cached_content_token_count: Cached content tokens
   - tool_use_prompt_token_count: Tool-related tokens
   - thoughts_token_count: Thinking tokens

Generation Configuration:

1. BASIC PARAMETERS:
   - temperature: 0.0-2.0 (randomness control)
   - max_output_tokens: Maximum response length
   - top_p: Nucleus sampling threshold
   - top_k: Top-k sampling limit
   - seed: Reproducible results
   - candidate_count: Number of responses

2. STRUCTURED OUTPUT:
   - response_mime_type: "application/json", "text/plain"
   - response_schema: Pydantic model or schema
   - response_json_schema: JSON Schema format

3. ADVANCED FEATURES:
   - stop_sequences: Stop generation triggers
   - presence_penalty: Token reuse penalty
   - frequency_penalty: Repeated token penalty
   - response_logprobs: Include probability data
   - thinking_config: Enable reasoning capabilities

4. SAFETY SETTINGS:
   - See detailed safety settings section below

5. CORRECT API STRUCTURE (Updated):
```python
# For newer google-genai API, use config parameter
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="Your prompt",
    config={
        "temperature": 0.1,
        "max_output_tokens": 1000,
        "response_mime_type": "application/json",
        "response_schema": your_schema
    }
    # Note: safety_settings, system_instruction, etc. go in config
)
```

6. CRITICAL API DIFFERENCES DISCOVERED:
   - ❌ WRONG: `generation_config=types.GenerateContentConfig(...)`
   - ✅ CORRECT: `config={...}` (dictionary format)
   - ❌ WRONG: `safety_settings=[...]` (as separate parameter)
   - ✅ CORRECT: `config={"safety_settings": [...]}` (inside config)
   - ❌ WRONG: `system_instruction="..."` (as separate parameter)
   - ✅ CORRECT: `config={"system_instruction": "..."}` (inside config)

7. COMMON MISTAKES TO AVOID:
   - Don't use `types.GenerateContentConfig()` - use plain dictionary
   - Don't pass parameters separately - put everything in `config={}`
   - The newer API expects a simpler, flatter structure
   - Always test with basic generation first before adding complexity

Best Practices:

1. RESPONSE HANDLING:
   - Always check finish_reason for completion status
   - Handle safety blocks gracefully
   - Monitor token usage for cost control
   - Use streaming for long responses

2. ERROR HANDLING:
   - Check prompt_feedback for blocked content
   - Handle rate limits and quotas
   - Implement retry logic with backoff
   - Validate response structure

3. PERFORMANCE OPTIMIZATION:
   - Use appropriate model for task complexity
   - Cache content when possible
   - Optimize prompt length
   - Monitor token usage patterns

4. SAFETY AND COMPLIANCE:
   - See detailed safety settings section below

================================================================================
SAFETY SETTINGS - Content Filtering and Harm Prevention: https://ai.google.dev/gemini-api/docs/safety-settings
================================================================================

What are Safety Settings?
Safety settings are adjustable filters that control what types of content the Gemini API allows or blocks. These settings help ensure appropriate content for your application while protecting users from harmful content.

Safety Filter Categories:

1. HARM_CATEGORY_HARASSMENT:
   - Negative or harmful comments targeting identity and/or protected attributes
   - Blocks content that targets individuals or groups based on characteristics

2. HARM_CATEGORY_HATE_SPEECH:
   - Content that is rude, disrespectful, or profane
   - Blocks discriminatory or offensive language

3. HARM_CATEGORY_SEXUALLY_EXPLICIT:
   - Contains references to sexual acts or other lewd content
   - Blocks inappropriate sexual content

4. HARM_CATEGORY_DANGEROUS_CONTENT:
   - Promotes, facilitates, or encourages harmful acts
   - Blocks content that could cause real-world harm

5. HARM_CATEGORY_CIVIC_INTEGRITY:
   - Election-related queries and content
   - Blocks content that could interfere with democratic processes

Content Safety Probability Levels:
- HIGH: High probability of being unsafe
- MEDIUM: Medium probability of being unsafe  
- LOW: Low probability of being unsafe
- NEGLIGIBLE: Negligible probability of being unsafe

Block Threshold Settings:

1. BLOCK_NONE:
   - Always show content regardless of probability
   - Most permissive setting
   - Use with extreme caution

2. BLOCK_ONLY_HIGH:
   - Block only high probability unsafe content
   - Allows medium, low, and negligible content

3. BLOCK_MEDIUM_AND_ABOVE:
   - Block medium and high probability content
   - Default setting for most models
   - Balanced approach

4. BLOCK_LOW_AND_ABOVE:
   - Block low, medium, and high probability content
   - Most restrictive setting
   - Maximum safety

Implementation Examples:

1. Basic Safety Settings:
```python
from google import genai
from google.genai import types

client = genai.Client()
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="Your prompt here",
    config=types.GenerateContentConfig(
        safety_settings=[
            types.SafetySetting(
                category=types.HarmCategory.HARM_CATEGORY_HATE_SPEECH,
                threshold=types.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
            ),
            types.SafetySetting(
                category=types.HarmCategory.HARM_CATEGORY_HARASSMENT,
                threshold=types.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
            )
        ]
    )
)
```

2. Per-Category Customization:
```python
safety_settings = [
    types.SafetySetting(
        category=types.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
        threshold=types.HarmBlockThreshold.BLOCK_ONLY_HIGH,  # Allow more for gaming
    ),
    types.SafetySetting(
        category=types.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
        threshold=types.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,  # Strict for family app
    )
]
```

3. Response Safety Handling:
```python
response = client.models.generate_content(...)

# Check if content was blocked
if response.candidates[0].finish_reason == types.FinishReason.SAFETY:
    print("Content was blocked for safety reasons")
    
    # Check specific safety ratings
    for rating in response.candidates[0].safety_ratings:
        print(f"Blocked: {rating.category} - {rating.probability}")
```

4. Prompt Safety Feedback:
```python
# Check if prompt was blocked
if response.prompt_feedback.block_reason:
    print(f"Prompt blocked: {response.prompt_feedback.block_reason}")
    
    # Check prompt safety ratings
    for rating in response.prompt_feedback.safety_ratings:
        print(f"Prompt issue: {rating.category} - {rating.probability}")
```

Default Settings by Model:

1. Newer Models (gemini-2.0+):
   - Default: BLOCK_NONE for most categories
   - Civic integrity: BLOCK_NONE

2. Older Models:
   - Default: BLOCK_MEDIUM_AND_ABOVE for most categories
   - Civic integrity: BLOCK_MOST

Best Practices:

1. APPLICATION-SPECIFIC SETTINGS:
   - Gaming: Allow more dangerous content (BLOCK_ONLY_HIGH)
   - Educational: Moderate settings (BLOCK_MEDIUM_AND_ABOVE)
   - Family apps: Strict settings (BLOCK_LOW_AND_ABOVE)
   - Professional: Balanced approach (BLOCK_MEDIUM_AND_ABOVE)

2. TESTING AND VALIDATION:
   - Test with edge cases
   - Monitor false positives/negatives
   - Adjust based on user feedback
   - Document your safety approach

3. ERROR HANDLING:
   - Always check finish_reason for SAFETY
   - Provide user-friendly error messages
   - Log safety blocks for monitoring
   - Implement fallback responses

4. COMPLIANCE CONSIDERATIONS:
   - Review Terms of Service
   - Consider regional regulations
   - Document safety decisions
   - Regular policy reviews

5. MONITORING:
   - Track safety block rates
   - Monitor user complaints
   - Analyze blocked content patterns
   - Adjust settings based on data

Important Notes:
- Built-in protections against core harms (child safety) cannot be disabled
- Less restrictive settings may trigger review
- Always test thoroughly before production
- Consider legal and ethical implications
- Monitor and adjust settings regularly

TURNING OFF SAFETY SETTINGS:

1. Method 1: Set threshold to BLOCK_NONE
```python
safety_settings = [
    types.SafetySetting(
        category=types.HarmCategory.HARM_CATEGORY_HATE_SPEECH,
        threshold=types.HarmBlockThreshold.BLOCK_NONE
    ),
    types.SafetySetting(
        category=types.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
        threshold=types.HarmBlockThreshold.BLOCK_NONE
    ),
    types.SafetySetting(
        category=types.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
        threshold=types.HarmBlockThreshold.BLOCK_NONE
    ),
    types.SafetySetting(
        category=types.HarmCategory.HARM_CATEGORY_HARASSMENT,
        threshold=types.HarmBlockThreshold.BLOCK_NONE
    )
]
```

2. Method 2: Use OFF threshold (completely disable)
```python
safety_settings = [
    types.SafetySetting(
        category=types.HarmCategory.HARM_CATEGORY_HATE_SPEECH,
        threshold=types.HarmBlockThreshold.OFF
    )
]
```

3. Apply to request (correct syntax for newer API):
```python
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="Your prompt",
    config={
        "safety_settings": safety_settings,
        "temperature": 0.1
    }
)
```

⚠️ WARNING: Disabling safety settings may violate Terms of Service and could result in account review. Use only when absolutely necessary and with extreme caution.

================================================================================
FILES API - Working with Media Files and Documents: https://ai.google.dev/gemini-api/docs/files
================================================================================

What is the Files API?
The Files API allows you to upload and work with various media files (images, audio, video, documents) in the Gemini API. This is essential for multimodal content generation and when request sizes exceed 20 MB.

Key Features:
- Support for multiple file types (images, audio, video, documents, PDFs)
- Automatic file deletion after 48 hours
- Up to 20 GB storage per project
- Maximum file size: 2 GB per file
- No cost for file storage

Basic File Operations:

1. UPLOAD A FILE:
```python
from google import genai

client = genai.Client()

# Upload a file
myfile = client.files.upload(file="path/to/sample.pdf")

# Use the file in content generation
response = client.models.generate_content(
    model="gemini-2.5-flash", 
    contents=["Extract information from this document", myfile]
)

print(response.text)
```

2. GET FILE METADATA:
```python
# Upload and get metadata
myfile = client.files.upload(file='path/to/sample.pdf')
file_name = myfile.name

# Get file information
file_info = client.files.get(name=file_name)
print(file_info)
```

3. LIST UPLOADED FILES:
```python
# List all uploaded files
print('My files:')
for f in client.files.list():
    print(' ', f.name)
```

4. DELETE UPLOADED FILES:
```python
# Upload and then delete
myfile = client.files.upload(file='path/to/sample.pdf')
client.files.delete(name=myfile.name)
```

File Prompting Strategies:

1. PROMPT DESIGN FUNDAMENTALS:
   - Be specific in your instructions
   - Add few-shot examples to illustrate desired output
   - Break complex tasks into step-by-step instructions
   - Specify the output format (JSON, markdown, HTML, etc.)
   - Put image first for single-image prompts

2. TROUBLESHOOTING MULTIMODAL PROMPTS:

   A. If model isn't drawing from relevant parts:
   - Drop hints about which aspects to focus on
   - Point out specific elements in the image/document

   B. If output is too generic:
   - Ask model to describe the image/document first
   - Explicitly ask to refer to what's in the image
   - Use "Refer to what's in the image" in your prompt

   C. If content is hallucinated:
   - Lower temperature setting
   - Ask for shorter descriptions
   - Be more specific about what to extract

   D. To troubleshoot failures:
   - Ask model to describe the image/document first
   - Ask model to explain its reasoning
   - Check if model understood the input correctly

3. BEST PRACTICES FOR FILE PROMPTS:

   A. Be Specific in Instructions:
   ```python
   # Instead of: "Describe this image"
   # Use: "Parse the time and city from the airport board shown in this image into a list"
   ```

   B. Add Few-Shot Examples:
   ```python
   # Provide examples of desired output format
   contents = [
       "Extract city and landmark from this image:",
       example_image_1,
       "city: Rome, landmark: the Colosseum",
       example_image_2,
       "city: Beijing, landmark: Forbidden City",
       target_image,
       "Now extract from this image:"
   ]
   ```

   C. Break Down Complex Tasks:
   ```python
   # Instead of: "When will I run out of toilet paper?"
   # Use: "1. Count the toilet paper rolls. 2. Calculate daily usage. 3. Determine duration."
   ```

   D. Specify Output Format:
   ```python
   # For structured output
   contents = [
       "Parse this table into JSON format with columns: name, age, city",
       table_image
   ]
   ```

   E. Put Image First:
   ```python
   # For single-image prompts, put image before text
   contents = [image_file, "Extract information from this document"]
   ```

4. STRUCTURED OUTPUT WITH FILES:
```python
# Combine file upload with structured output
myfile = client.files.upload(file="path/to/document.pdf")

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=["Extract CAO information from this PDF", myfile],
    config={
        "response_mime_type": "application/json",
        "response_schema": CAOExtractionSchema,
        "temperature": 0.1
    }
)
```

5. ERROR HANDLING:
```python
try:
    myfile = client.files.upload(file="path/to/file.pdf")
    
    # Check if file is ready
    file_info = client.files.get(name=myfile.name)
    if file_info.state.name == "ACTIVE":
        # File is ready to use
        response = client.models.generate_content(...)
    else:
        print("File not ready yet")
        
except Exception as e:
    print(f"File upload error: {e}")
```

Usage Guidelines:
- Use Files API when total request size > 20 MB
- Files are automatically deleted after 48 hours
- Maximum 2 GB per file, 20 GB total per project
- No cost for file storage
- Available in all regions where Gemini API is available
